{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST from scratch with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load changed modules automatically\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load numpy\n",
    "import numpy as np\n",
    "\n",
    "# load dataloaders and required layers\n",
    "from mnist import dataloader\n",
    "from mnist import layers\n",
    "from mnist.layers import Softmax, Linear\n",
    "\n",
    "# load pyplot for displaying images\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# show images inline on notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# debugging\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = dataloader.DataLoader()\n",
    "((x_train, y_train), (x_valid, y_valid), _) = dl.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 784), (50000,), (10000, 784), (10000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_valid, y_valid) = dl.normalize(((x_train, y_train), (x_valid, y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show(img):\n",
    "    plt.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "valid_images = np.reshape(x_valid, (-1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show(valid_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(arr1, arr2):\n",
    "    random_idxs = np.arange(len(arr1))\n",
    "    np.random.shuffle(random_idxs)\n",
    "    return x_train[random_idxs], y_train[random_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always reproduce the same weights\n",
    "#np.random.seed(1)\n",
    "\n",
    "class Net():\n",
    "    def __init__(self):\n",
    "        self.input_layer = Linear(28*28, 10) # linear layer with bias\n",
    "        self.softmax = Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer.forward(x)\n",
    "        x = self.softmax.forward(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.076543342103366 1.6804543203751925 0.6101190476190477 0.49609375\n",
      "1 1.9110294050639973 1.5860954056228453 0.6875 0.578125\n",
      "2 1.831531643722002 1.4208547872320407 0.6964285714285714 0.650390625\n",
      "3 1.7929787882997836 1.7427525296464241 0.7172619047619048 0.6171875\n",
      "4 1.7722957456274895 1.7972089727254121 0.7232142857142857 0.61328125\n",
      "5 1.76007463343865 1.8872505570209188 0.7232142857142857 0.634765625\n",
      "6 1.7522785946222765 2.0790246460075528 0.7261904761904762 0.6484375\n",
      "7 1.747018177700156 2.010945473784499 0.7261904761904762 0.6640625\n",
      "8 1.7433241831559356 2.4057902377220644 0.7261904761904762 0.64453125\n",
      "9 1.740653141903805 2.921471892649608 0.7261904761904762 0.59765625\n",
      "10 1.738676820059311 3.199908242079837 0.7261904761904762 0.623046875\n",
      "11 1.7371859206723423 3.53180765122415 0.7261904761904762 0.6171875\n",
      "12 1.73604184339761 3.3357648507637627 0.7261904761904762 0.642578125\n",
      "13 1.7351502663861553 3.507613658283627 0.7261904761904762 0.6171875\n",
      "14 1.7344456216911586 3.81612945485708 0.7232142857142857 0.6640625\n",
      "15 1.7338815097621645 4.266357740045585 0.7232142857142857 0.646484375\n",
      "16 1.7334245758777376 4.44049702975375 0.7232142857142857 0.623046875\n",
      "17 1.7330504994586657 4.400441817426706 0.7202380952380952 0.630859375\n",
      "18 1.7327413142760162 4.328510045634203 0.7202380952380952 0.650390625\n",
      "19 1.7324835867430715 5.494075686396825 0.7202380952380952 0.615234375\n",
      "20 1.7322671580207845 5.289592528507494 0.7202380952380952 0.6484375\n",
      "21 1.7320842628178883 5.825505963986197 0.7202380952380952 0.630859375\n",
      "22 1.7319289037957726 5.200179754174998 0.7202380952380952 0.63671875\n",
      "23 1.731796401980463 6.064740428041519 0.7202380952380952 0.615234375\n",
      "24 1.731683070077313 5.883477398664189 0.7202380952380952 0.63671875\n",
      "25 1.7315859727435086 7.024211662795574 0.7202380952380952 0.630859375\n",
      "26 1.731502749140478 7.4779607900590594 0.7202380952380952 0.609375\n",
      "27 1.7314314805875501 7.388426773339714 0.7202380952380952 0.6328125\n",
      "28 1.731370591198814 6.678199206538042 0.7232142857142857 0.677734375\n",
      "29 1.7313187728479105 7.355099861841303 0.7232142857142857 0.65234375\n",
      "30 1.731274928207916 7.7839031207339175 0.7232142857142857 0.6484375\n",
      "31 1.7312381273029336 7.617713993179264 0.7232142857142857 0.638671875\n",
      "32 1.7312075742113915 7.891817905890231 0.7232142857142857 0.640625\n",
      "33 1.7311825814284683 7.667263392660484 0.7232142857142857 0.638671875\n",
      "34 1.7311625500270569 9.058304602951473 0.7232142857142857 0.634765625\n",
      "35 1.7311469542214486 8.29342365678708 0.7232142857142857 0.650390625\n",
      "36 1.7311353292824656 8.641975111107879 0.7232142857142857 0.626953125\n",
      "37 1.731127262009942 7.3456807618192705 0.7232142857142857 0.677734375\n",
      "38 1.7311223831613147 9.662476439002262 0.7232142857142857 0.6171875\n",
      "39 1.7311203613804835 8.519118208171559 0.7232142857142857 0.67578125\n",
      "40 1.7311208982808786 11.408311289195664 0.7232142857142857 0.6015625\n",
      "41 1.7311237244198996 10.394336030794967 0.7232142857142857 0.60546875\n",
      "42 1.731128595964946 9.289438461432047 0.7232142857142857 0.66796875\n",
      "43 1.731135291899136 8.635422645289841 0.7261904761904762 0.681640625\n",
      "44 1.7311436116511427 12.711428296655232 0.7261904761904762 0.6015625\n",
      "45 1.7311533730611433 10.725921687531468 0.7261904761904762 0.642578125\n",
      "46 1.73116441061577 10.465988230981019 0.7261904761904762 0.65234375\n",
      "47 1.7311765739007736 11.17554324359996 0.7261904761904762 0.6640625\n",
      "48 1.7311897262320943 12.0983855168799 0.7261904761904762 0.607421875\n",
      "49 1.7312037434350689 10.56448350270172 0.7261904761904762 0.6640625\n"
     ]
    }
   ],
   "source": [
    "debug = False\n",
    "\n",
    "if debug:\n",
    "    n_epochs = 1\n",
    "    batch_size = 8\n",
    "else:\n",
    "    n_epochs = 50\n",
    "    batch_size = 512\n",
    "    \n",
    "learning_rate = 1e-3\n",
    "prev_loss = None\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    for i in range(0, len(x_train), batch_size):\n",
    "\n",
    "        inputs = x_train[i:i+batch_size]\n",
    "        targets = y_train[i:i+batch_size]\n",
    "        \n",
    "        inputs, targets = shuffle(inputs, targets)\n",
    "            \n",
    "        if debug:\n",
    "            print(\"inputs.shape\", inputs.shape)\n",
    "            print(\"targets.shape\", targets.shape)\n",
    "\n",
    "        # forward propagation\n",
    "        y_pred = net.forward(inputs)\n",
    "        predictions = net.softmax.forward(y_pred)\n",
    "\n",
    "        if debug:\n",
    "            print(\"y_pred.shape:\", y_pred.shape)\n",
    "            print(\"predictions.shape\", predictions.shape)\n",
    "            \n",
    "        # calculate cross-entropy loss\n",
    "        loss = net.softmax.cross_entropy(predictions, targets)\n",
    "        \n",
    "        if debug:\n",
    "            print(epoch, loss)\n",
    "        \n",
    "        # backpropagation        \n",
    "        grad_loss = net.softmax.backward(predictions, targets)\n",
    "        grad_input_layer, grad_bias = net.input_layer.backward(inputs, grad_loss)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"grad_input_layer.shape\", grad_input_layer.shape)\n",
    "            print(\"net.input_layer.weights.shape\", net.input_layer.weights.shape)\n",
    "            print(\"net.input_layer.bias.shape\", net.input_layer.bias.shape)\n",
    "        \n",
    "        net.input_layer.weights -= learning_rate * grad_input_layer.T\n",
    "        net.input_layer.bias -= learning_rate * grad_bias\n",
    "        \n",
    "        if debug:\n",
    "            break\n",
    "        \n",
    "    # calculate validation loss for some random indices\n",
    "    random_idxs = np.random.randint(0, len(x_valid), batch_size)\n",
    "    y_valid_pred = net.forward(x_valid[random_idxs])\n",
    "    loss_valid = net.softmax.cross_entropy(y_valid_pred, y_valid[random_idxs])\n",
    "    \n",
    "    #calculate accuracy and validation accuracy\n",
    "    accuracy = np.mean(y_pred.argmax(axis=1) == targets)\n",
    "    valid_accuracy = np.mean(y_valid_pred.argmax(axis=1) == y_valid[random_idxs])\n",
    "    \n",
    "    print(epoch, loss, loss_valid, accuracy, valid_accuracy)\n",
    "\n",
    "    if prev_loss is None:\n",
    "        prev_loss = loss\n",
    "    else:\n",
    "        if loss > prev_loss:\n",
    "            print(\"EPOCH\", epoch, \"ITS BIGGER\")\n",
    "            #pdb.set_trace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
