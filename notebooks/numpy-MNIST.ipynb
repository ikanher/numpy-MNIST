{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST from scratch with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load changed modules automatically\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load numpy\n",
    "import numpy as np\n",
    "\n",
    "# load dataloaders and required layers\n",
    "from mnist import dataloader\n",
    "from mnist.layers import Softmax, Linear, Dropout, ReLU\n",
    "from mnist.losses import CrossEntropy\n",
    "\n",
    "# load pyplot for displaying images\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# show images inline on notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# debugging\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = dataloader.DataLoader()\n",
    "((x_train, y_train), (x_valid, y_valid), (x_test, (y_test))) = dl.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 784), (50000,), (10000, 784), (10000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_valid, y_valid) = dl.normalize(((x_train, y_train), (x_valid, y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show(img):\n",
    "    plt.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "valid_images = np.reshape(x_valid, (-1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show(valid_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(arr1, arr2):\n",
    "    random_idxs = np.arange(len(arr1))\n",
    "    np.random.shuffle(random_idxs)\n",
    "    return x_train[random_idxs], y_train[random_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always reproduce the same weights\n",
    "np.random.seed(1)\n",
    "\n",
    "class Net(object):\n",
    "    def __init__(self):\n",
    "        self.train = True\n",
    "        self.input_layer = Linear(28*28, 10) # linear layer with bias\n",
    "        self.softmax = Softmax()\n",
    "        self.dropout = Dropout(0.7)\n",
    "        self.cross_entropy = CrossEntropy()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.train:\n",
    "            x = self.dropout.forward(x)\n",
    "\n",
    "        x = self.input_layer.forward(x)\n",
    "        x = self.softmax.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, targets):\n",
    "        grad_loss = net.softmax.backward(targets)\n",
    "        _, grad_input_layer, grad_bias = net.input_layer.backward(grad_loss)\n",
    "        return grad_input_layer, grad_bias\n",
    "    \n",
    "    def loss(self, y_pred, y):\n",
    "        return self.cross_entropy.loss(y_pred, y)\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5394824091541721 0.6910538516122477 0.8720238095238095 0.814453125\n",
      "1 0.4174654855262794 0.5461995047442303 0.8958333333333334 0.822265625\n",
      "2 0.38805430993767315 0.5323003747201825 0.8869047619047619 0.8203125\n",
      "3 0.36190649005957926 0.41800528465176945 0.9017857142857143 0.88671875\n",
      "4 0.33190516304940043 0.47453546980488276 0.9136904761904762 0.865234375\n",
      "5 0.3527439663648609 0.41713942475530386 0.8869047619047619 0.8828125\n",
      "6 0.3077765987682525 0.4526116126184475 0.9315476190476191 0.876953125\n",
      "7 0.3058923074747741 0.5035840627165902 0.9077380952380952 0.84375\n",
      "8 0.29052231244484644 0.5003601816389651 0.9017857142857143 0.849609375\n",
      "9 0.25314776921203763 0.5094530495957906 0.9375 0.83984375\n",
      "10 0.25548083162702695 0.4219635189079232 0.9255952380952381 0.857421875\n",
      "11 0.2552913826929397 0.4545912315986147 0.9255952380952381 0.84375\n",
      "12 0.23288145100753124 0.5024954138355286 0.9375 0.84765625\n",
      "13 0.24670659362628983 0.5159597887466021 0.9315476190476191 0.8515625\n",
      "14 0.22228903693179558 0.4052670112710162 0.9345238095238095 0.865234375\n",
      "15 0.1963105580436196 0.5541728227642111 0.9523809523809523 0.837890625\n",
      "16 0.2341740826008239 0.5318200036110299 0.9315476190476191 0.859375\n",
      "17 0.24401908966115762 0.48355315278147176 0.9285714285714286 0.85546875\n",
      "18 0.19781691734096793 0.4079479944620097 0.9494047619047619 0.875\n",
      "19 0.22862114252781773 0.42274163616679256 0.9255952380952381 0.875\n",
      "20 0.2410013053641802 0.44532591020171153 0.9166666666666666 0.873046875\n",
      "21 0.19309894184394746 0.5451238211024285 0.9345238095238095 0.85546875\n",
      "22 0.19579628317374245 0.6363364486704698 0.9404761904761905 0.830078125\n",
      "23 0.2233182570439523 0.4690527765205398 0.9196428571428571 0.86328125\n",
      "24 0.2245799716831461 0.44874254138404945 0.9345238095238095 0.884765625\n",
      "25 0.19790964074454834 0.5077588535367046 0.9494047619047619 0.853515625\n",
      "26 0.1927601956724418 0.5279974089833767 0.9464285714285714 0.8359375\n",
      "27 0.22398077784144657 0.3701492206099346 0.9315476190476191 0.884765625\n",
      "28 0.21364320447205123 0.5240738649050073 0.9404761904761905 0.833984375\n",
      "29 0.1887859981338916 0.4882108822732203 0.9375 0.87109375\n",
      "30 0.22233107324104887 0.5125399248216195 0.9375 0.826171875\n",
      "31 0.2358921475349421 0.5043006654847264 0.9375 0.84765625\n",
      "32 0.183719819428423 0.5274041173222596 0.9285714285714286 0.853515625\n",
      "33 0.15980902151263066 0.44357300522726173 0.9434523809523809 0.865234375\n",
      "34 0.20102544720774157 0.5961141805787737 0.9375 0.8359375\n",
      "35 0.20176501063560315 0.4588532412231044 0.9434523809523809 0.857421875\n",
      "36 0.1544479448594799 0.39021283117907735 0.9553571428571429 0.888671875\n",
      "37 0.2135462171058317 0.4980661430246168 0.9404761904761905 0.880859375\n",
      "38 0.16864845109776042 0.543258298958108 0.9523809523809523 0.84765625\n",
      "39 0.2076579615954826 0.42863648472048954 0.9404761904761905 0.875\n",
      "40 0.1908154480219619 0.4404927969001959 0.9434523809523809 0.865234375\n",
      "41 0.1946590987079321 0.5338730396171092 0.9523809523809523 0.861328125\n"
     ]
    }
   ],
   "source": [
    "debug = False\n",
    "\n",
    "if debug:\n",
    "    n_epochs = 1\n",
    "    batch_size = 3\n",
    "else:\n",
    "    n_epochs = 50\n",
    "    batch_size = 512\n",
    "    \n",
    "learning_rate = 1e-2\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    for i in range(0, len(x_train), batch_size):\n",
    "\n",
    "        inputs = x_train[i:i+batch_size]\n",
    "        targets = y_train[i:i+batch_size]\n",
    "        \n",
    "        inputs, targets = shuffle(inputs, targets)\n",
    "\n",
    "        if debug:\n",
    "            print(\"inputs.shape\", inputs.shape)\n",
    "            print(\"targets.shape\", targets.shape)\n",
    "\n",
    "        # forward propagation\n",
    "        y_pred = net.forward(inputs)\n",
    "        predictions = y_pred.copy()\n",
    "\n",
    "        if debug:\n",
    "            print(\"y_pred.shape:\", y_pred.shape)\n",
    "            print(\"predictions.shape\", predictions.shape)\n",
    "            \n",
    "        # calculate cross-entropy loss\n",
    "        loss = net.loss(predictions, targets)\n",
    "        \n",
    "        if debug:\n",
    "            print(epoch, loss)\n",
    "        \n",
    "        # backpropagation\n",
    "        grad_input_layer, grad_bias = net.backward(targets)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"grad_input_layer.shape\", grad_input_layer.shape)\n",
    "            print(\"net.input_layer.weights.shape\", net.input_layer.weights.shape)\n",
    "            print(\"net.input_layer.bias.shape\", net.input_layer.bias.shape)\n",
    "        \n",
    "        net.input_layer.weights -= learning_rate * grad_input_layer\n",
    "        net.input_layer.bias -= learning_rate * grad_bias\n",
    "        \n",
    "        if debug:\n",
    "            break\n",
    "        \n",
    "    # calculate validation loss for some random indices\n",
    "    net.train = False\n",
    "    random_idxs = np.random.randint(0, len(x_valid), batch_size)\n",
    "    y_valid_pred = net.forward(x_valid[random_idxs])\n",
    "    loss_valid = net.loss(y_valid_pred, y_valid[random_idxs])\n",
    "    net.train = True\n",
    "    \n",
    "    #calculate accuracy and validation accuracy\n",
    "    accuracy = np.mean(y_pred.argmax(axis=1) == targets)\n",
    "    valid_accuracy = np.mean(y_valid_pred.argmax(axis=1) == y_valid[random_idxs])\n",
    "    \n",
    "    print(epoch, loss, loss_valid, accuracy, valid_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random examples from validation data,\n",
    "# compare predictions with actual values\n",
    "valid_preds = net.forward(x_valid)\n",
    "random_idxs = np.random.randint(0, len(x_valid), 10)\n",
    "np.argmax(valid_preds, axis=1)[random_idxs], y_valid[random_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a random image from validation data with\n",
    "# prediction and correct value\n",
    "valid_images = np.reshape(x_valid, (-1,28,28))\n",
    "valid_preds = net.forward(x_valid)\n",
    "random_idx = np.random.randint(0, len(x_valid))\n",
    "prediction = np.argmax(valid_preds, axis=1)[random_idx]\n",
    "correct = y_valid[random_idx]\n",
    "print(\"prediction:\", prediction, \"correct:\", correct)\n",
    "show(valid_images[random_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
