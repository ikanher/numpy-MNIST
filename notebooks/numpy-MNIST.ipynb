{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST from scratch with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# load changed modules automatically\n",
    "# XXX: doesn't seem to work, fix this\n",
    "%load_ext autoreload\n",
    "%autoreload = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load numpy\n",
    "import numpy as np\n",
    "\n",
    "# load dataloaders and required layers\n",
    "from mnist import dataloader\n",
    "from mnist import layers\n",
    "from mnist.layers import Softmax, Linear\n",
    "\n",
    "# load pyplot for displaying images\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# show images inline on notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# debugging\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = dataloader.DataLoader()\n",
    "((x_train, y_train), (x_valid, y_valid), _) = dl.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 784), (50000,), (10000, 784), (10000,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_valid, y_valid) = dl.normalize(((x_train, y_train), (x_valid, y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show(img):\n",
    "    plt.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "valid_images = np.reshape(x_valid, (-1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show(valid_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(arr1, arr2):\n",
    "    random_idxs = np.arange(len(arr1))\n",
    "    np.random.shuffle(random_idxs)\n",
    "    return x_train[random_idxs], y_train[random_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layer1 = Linear(28*28, 10) # linear layer with bias\n",
    "        self.softmax = Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1.forward(x)\n",
    "        x = self.softmax.forward(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30.163867417706467 57.75674351750538 0.15 0.1015625\n",
      "1 57.07530770697142 60.9413864519667 0.15 0.171875\n",
      "EPOCH 1 ITS BIGGER\n",
      "2 84.09682128223903 140.2615951418866 0.15 0.1015625\n",
      "EPOCH 2 ITS BIGGER\n",
      "3 111.18219486323062 148.38698424889355 0.15 0.1171875\n",
      "EPOCH 3 ITS BIGGER\n",
      "4 138.30605967027725 146.41257653072802 0.15 0.1640625\n",
      "EPOCH 4 ITS BIGGER\n",
      "5 165.4528919005424 222.3960714939671 0.15 0.1015625\n",
      "EPOCH 5 ITS BIGGER\n",
      "6 192.61351504907424 286.70239640631564 0.15 0.140625\n",
      "EPOCH 6 ITS BIGGER\n",
      "7 219.7825109734018 240.26942109218515 0.15 0.1328125\n",
      "EPOCH 7 ITS BIGGER\n",
      "8 246.95664082206696 nan 0.15 0.0859375\n",
      "EPOCH 8 ITS BIGGER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akir/school/numpy-MNIST/notebooks/mnist/layers.py:49: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(x)/np.sum(np.exp(x))\n",
      "/home/akir/school/numpy-MNIST/notebooks/mnist/layers.py:49: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(x)/np.sum(np.exp(x))\n",
      "/home/akir/school/numpy-MNIST/notebooks/mnist/layers.py:67: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.mean(np.log(out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 nan nan 0.125 0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akir/anaconda3/envs/deep-learning/lib/python3.6/site-packages/numpy/core/fromnumeric.py:83: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 128\n",
    "learning_rate = 1e-6\n",
    "debug = False\n",
    "prev_loss = None\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    for i in range(0, len(x_train), batch_size):\n",
    "\n",
    "        n = batch_size\n",
    "        inputs = x_train[i:i+n]\n",
    "        targets = y_train[i:i+n]\n",
    "        \n",
    "        inputs, targets = shuffle(inputs, targets)\n",
    "            \n",
    "        if debug:\n",
    "            print(\"inputs.shape\", inputs.shape)\n",
    "            print(\"targets.shape\", targets.shape)\n",
    "\n",
    "        # forward propagation\n",
    "        y_pred = net.forward(inputs)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"y_pred.shape:\", y_pred.shape)\n",
    "        \n",
    "        # calculate cross-entropy loss\n",
    "        loss = net.softmax.cross_entropy(y_pred, targets)\n",
    "        \n",
    "        if debug:\n",
    "            print(epoch, loss)\n",
    "        \n",
    "        # backpropagation\n",
    "        \n",
    "        # get the predictions\n",
    "        predictions = np.diag(y_pred[:, targets])\n",
    "        grad_loss = predictions - targets\n",
    "        if debug:\n",
    "            print(\"predictions.shape\", predictions.shape)\n",
    "            print(\"grad_loss.shape\", grad_loss.shape)\n",
    "        \n",
    "        # compute layer1 gradient\n",
    "        # XXX: How???\n",
    "        grad_layer1 = inputs.T @ grad_loss\n",
    "        grad_bias = np.mean(grad_loss)\n",
    "        \n",
    "        # XXX: Figure out how to implement this, maybe it's easier when\n",
    "        #      just calling layers' backward method...\n",
    "        #grad_layer1, grad_bias = net.layer1.backward(inputs)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"grad_layer1.shape\", grad_layer1.shape)\n",
    "            print(\"net.layer1.weights.shape\", net.layer1.weights.shape)\n",
    "            print(\"net.layer1.bias.shape\", net.layer1.bias.shape)\n",
    "        \n",
    "        net.layer1.weights -= learning_rate * grad_layer1\n",
    "        net.layer1.bias -= learning_rate * grad_bias\n",
    "        \n",
    "    # calculate validation loss for some random indices\n",
    "    random_idxs = np.random.randint(0, len(x_valid), batch_size)\n",
    "    y_valid_pred = net.forward(x_valid[random_idxs])\n",
    "    loss_valid = net.softmax.cross_entropy(y_valid_pred, y_valid[random_idxs])\n",
    "    \n",
    "    #calculate accuracy and validation accuracy\n",
    "    accuracy = np.mean(y_pred.argmax(axis=1) == targets)\n",
    "    valid_accuracy = np.mean(y_valid_pred.argmax(axis=1) == y_valid[random_idxs])\n",
    "    \n",
    "    print(epoch, loss, loss_valid, accuracy, valid_accuracy)\n",
    "\n",
    "    if prev_loss is None:\n",
    "        prev_loss = loss\n",
    "    else:\n",
    "        if loss > prev_loss:\n",
    "            print(\"EPOCH\", epoch, \"ITS BIGGER\")\n",
    "            #pdb.set_trace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
