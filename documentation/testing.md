# Testing

The program is tested with unit tests, integration tests, standard Machine Learning testing and usage testing.

## Unit testing

The individual parts of the program (dataloader, layers, losses) are unit tested.

The forward pass is tested by using expected inputs and outputs. The correct inputs and outputs where either generated using PyTorch or found on internet sources like Wikipedia or online courses.

The backward pass is tested using [Gradient Checking](http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization). Code for Gradient Checking was borrowed from [CS231n Assignment 1](http://cs231n.github.io/assignments2016/assignment1/) and then applied for testing the backward pass of the individual layers.

Unit tests are build using the standard python [unittest library](https://docs.python.org/3/library/unittest.html).

### Running the tests

Tests can be run by using the [pytest](https://docs.pytest.org/en/latest/) command.

```
taivasnet$ pytest
========================================== test session starts ==========================================
platform linux -- Python 3.6.5, pytest-3.5.1, py-1.5.3, pluggy-0.6.0
rootdir: /home/akir/school/numpy-MNIST/taivasnet, inifile:
plugins: remotedata-0.2.1, openfiles-0.3.0, doctestplus-0.1.3, arraydiff-0.2
collected 20 items

test/test_dataloaders.py ..                                                                       [ 10%]
test/test_layers.py ..........                                                                    [ 60%]
test/test_losses.py .                                                                             [ 65%]
test/test_networks.py ....                                                                        [ 85%]
test/test_optimizers.py ...                                                                       [100%]

======================================= 20 passed in 8.30 seconds =======================================
```

### Code coverage

Coverage can be generated by running the [coverage](https://coverage.readthedocs.io/en/coverage-4.5.1a/) command.

You have to first run it once without coverage, so that it gets some weights to load.

```
$ ./train.py --epochs 1 --save
- Training model for 1 epoch, with learning rate 0.1
Epoch Train loss   Valid loss   Train acc Valid acc
0     1.9766170014 1.9213150915 0.2750000 0.2777000
- Saving weights to: ../data/saved_weights.dat
```

Now you can generate the coverage data

```
$ coverage run ./train.py --epochs 1 --save --load
- Loading weights from: ../data/saved_weights.dat
- Training model for 1 epoch, with learning rate 0.1
Epoch Train loss   Valid loss   Train acc Valid acc
0     0.4369344384 0.5027080413 0.8750000 0.8476000
- Saving weights to: ../data/saved_weights.dat
```

And get a report by running `coverage report`

```
$ coverage report
Name                       Stmts   Miss  Cover
----------------------------------------------
taivasnet/__init__.py          0      0   100%
taivasnet/dataloaders.py      27      2    93%
taivasnet/layers.py           70      3    96%
taivasnet/losses.py            7      0   100%
taivasnet/models.py           24      0   100%
taivasnet/networks.py         55      9    84%
taivasnet/optimizers.py       53      3    94%
train.py                      27      0   100%
----------------------------------------------
TOTAL                        263     17    94%
```

Also an HTML report can be generated by running

```
coverage html
```

It can be then viewed in the browser by opening the `htmlcov/index.html` file.

## Integration testing

### The optimizer
Integration tests are implemented for the Stochastic Gradient Descent (SGD) optimizer.

The test uses a subset of MNIST data and checks that the loss is getting lower after running a different number of optimizers steps.

### The neural network
For the NeuralNet the integration tests work in a similar manner. Two layer network and some random inputs are created. Then two linear layers are created and weights are copied from the two layer network.

Forward pass is checked by combining the results of the two linear layers chained together and then compared to the results given by the NeuralNet's forward() method. The backward pass is tested in a similar manner.

As each of the layers have their own unit-tests that are tested using Gradient Checking, this should give very good confidence that the forward and backward passes are working correctly.

## Machine Learning testing

MNIST database comes already divided in three sets. Training set, validation set and test set.

During the training it is very important that only training data is used. The results are then checked against validation set and the weights are adjusted accordingly.

Never during the building of the model or training the model the test set is used.

After the network is ready it is evaluated against the test set data. This give the final accuracy of the neural network model.

Test can be done by first training the network

```
$ ./train.py --epochs 20 --save
- Training model for 20 epoch, with learning rate 0.1
Epoch Train loss   Valid loss   Train acc Valid acc
0     1.5630066073 1.4939244801 0.4625000 0.4545000
1     0.4858543611 0.4722542815 0.8500000 0.8605000
2     0.2826413546 0.2965191751 0.9375000 0.9150000
3     0.1655711033 0.2195714570 0.9500000 0.9367000
4     0.1746750307 0.1758653760 0.9375000 0.9510000
5     0.1120270871 0.1469914880 0.9875000 0.9593000
6     0.1107354093 0.1320762823 0.9750000 0.9619000
7     0.0814344076 0.1144965976 0.9875000 0.9677000
8     0.0715982803 0.1077676272 0.9750000 0.9697000
9     0.0694175710 0.1027864482 0.9750000 0.9701000
10    0.0441153274 0.0937176742 1.0000000 0.9736000
11    0.0692730487 0.0895030952 0.9750000 0.9743000
12    0.0557225957 0.0867564994 0.9750000 0.9750000
13    0.0473420582 0.0843610814 1.0000000 0.9762000
14    0.0632664255 0.0815441377 0.9875000 0.9770000
15    0.0302250556 0.0779607758 1.0000000 0.9782000
16    0.0426277603 0.0753950672 0.9875000 0.9794000
17    0.0270402511 0.0762966677 1.0000000 0.9782000
18    0.0302901822 0.0773504286 0.9875000 0.9779000
19    0.0129810146 0.0712935601 1.0000000 0.9801000
- Saving weights to: ../data/saved_weights.dat
```

After that the final accuracies are printed out when running [predict.py](../taivasnet/predict.py) script as follows.

```
$ ./predict.py
Training set accuracy: 0.98342
Validation set accuracy: 0.9747
Test set accuracy: 0.9723
^C
```

Achieving 97% accuracy on the test set, using a relatively simple model, is a clear sign that the code and model are performing as expected.

## Usage testing

Last layer of testing is usage testing.

This was done by training the network and then using the `predict.py` script for manually checking that the predictions, actuals and the digit in the image all match most of the time.

