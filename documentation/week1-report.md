# Tira project report - Week 1

## What did I do?
* Deciced my project topic, creating a Neural Network for MNIST classifying, using just plain numpy
* Looked up MNIST data and ways to load it in
* Looked up a _LOT_ of documentation and researched through it
* Wrote a linear (no hidden layer) classifier with Pytorch in a Jupyter notebook, using as little Pytorch features as necessary (mainly autograd)
* Tested the above mentioned notebook that at least training a linear classifier shouldn't take too long on a CPU

## How has the program progressed?
Getting the Pytorch base classifier working on first week was pretty sweet. So I think pretty well and I'm starting to think the project is doable in the required timeframee.

## What did I learn?
* More Pytorch, which is pretty new to me
* More numpy
* More Python
* A _LOT_ about Neural Networks, especially trying to understand how the Cross-Entropy Loss function works and is implemented, not to mention how to backpropagate through it in this simplest linear network model
* More about math, derivatives and chain rule, which is a nice refresher after Differential Calculus -course

## What problems did I have?
* How to analyze the time and space complexity?
* I still do not completely understand Cross-Entropy loss function and its derivative
* I still do not nearly at all understand how to manually (instead of Pytorch autograd) to do the Backpropagation
* I'm first writing this thing in a Jupyter Notebook, so probably learning about Python testing and Code coveragee _might_ be some trouble at a later phase

## What next?
* Figure out how to use the chain rule when backpropagating
* Start converting code to plain numpy

## Hours
* Probably around 25

